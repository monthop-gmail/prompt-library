# üèõÔ∏è Government Data Collector Agent
## ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠ Prompt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏≤‡∏Ñ‡∏£‡∏±‡∏ê + ‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏õ‡πá‡∏ô Projects ‡∏î‡πâ‡∏ß‡∏¢ Claude Artifacts

---

## ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏£‡∏∞‡∏ö‡∏ö

```
‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏£‡∏±‡∏ê ‚îÄ‚îÄ‚îê
                        ‚îÇ
data.go.th (CKAN API) ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚ñ∂ [Scraper Agent] ‚îÄ‚îÄ‚ñ∂ [Cleaner / Parser] ‚îÄ‚îÄ‚ñ∂ [Project Storage]
                        ‚îÇ         ‚îÇ                      ‚îÇ                     ‚îÇ
BOT / SEC / DBD APIs ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ                      ‚îÇ                     ‚îÇ
                            Claude AI ‡∏ä‡πà‡∏ß‡∏¢          ‡∏à‡∏±‡∏î schema           Dashboard
                           ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á       ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î          ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ / Export
```

### ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏≤‡∏Ñ‡∏£‡∏±‡∏ê‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç

| ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• | URL | ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó | ‡∏ß‡∏¥‡∏ò‡∏µ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• |
|---|---|---|---|
| **data.go.th** | data.go.th | Open Data ‡∏Å‡∏•‡∏≤‡∏á | CKAN API |
| **‡∏ò‡∏õ‡∏ó. (BOT)** | bot.or.th | ‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô / ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡πÅ‡∏•‡∏Å‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô | BOT API |
| **‡∏Å‡∏•‡∏ï. (SEC)** | sec.or.th | ‡∏ï‡∏•‡∏≤‡∏î‡∏ó‡∏∏‡∏ô / ‡∏Å‡∏≠‡∏á‡∏ó‡∏∏‡∏ô | SEC API + Scrape |
| **‡∏Å‡∏£‡∏°‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à (DBD)** | opendata.dbd.go.th | ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏¥‡∏ï‡∏¥‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏• / ‡∏á‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô | DBD Open API |
| **‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì** | bb.go.th | ‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡πÅ‡∏ú‡πà‡∏ô‡∏î‡∏¥‡∏ô | Scrape (PDF/HTML) |
| **‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥** | nso.go.th | ‡∏™‡∏≥‡∏°‡∏∞‡πÇ‡∏ô‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏Å‡∏£ / ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥ | Download + Scrape |
| **‡∏Å‡∏£‡∏°‡∏≠‡∏∏‡∏ï‡∏∏‡∏ô‡∏¥‡∏¢‡∏°‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤** | tmd.go.th | ‡∏™‡∏†‡∏≤‡∏û‡∏≠‡∏≤‡∏Å‡∏≤‡∏® | API + Scrape |
| **‡∏™‡∏û‡∏£. (DGA)** | dga.or.th | ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ê‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏• | API Gateway |
| **‡∏£‡∏≤‡∏ä‡∏Å‡∏¥‡∏à‡∏à‡∏≤‡∏ô‡∏∏‡πÄ‡∏ö‡∏Å‡∏©‡∏≤** | ratchakitcha.soc.go.th | ‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢ / ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏® | Scrape (PDF) |
| **‡∏™‡∏†‡∏≤‡∏û‡∏±‡∏í‡∏ô‡πå (NESDC)** | nesdc.go.th | ‡πÅ‡∏ú‡∏ô‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏ä‡∏≤‡∏ï‡∏¥ / GDP | API + Download |

---

## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: Project Manager Dashboard (Artifact ‡∏´‡∏•‡∏±‡∏Å)

### Prompt 1.1 ‚Äî ‡∏™‡∏£‡πâ‡∏≤‡∏á Dashboard ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏≤‡∏Ñ‡∏£‡∏±‡∏ê

```
‡∏™‡∏£‡πâ‡∏≤‡∏á React artifact ‡πÄ‡∏õ‡πá‡∏ô "Government Data Project Manager" ‡∏ó‡∏µ‡πà‡∏°‡∏µ:

1. ‡∏´‡∏ô‡πâ‡∏≤ Projects:
   - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡πÉ‡∏´‡∏°‡πà (‡∏ä‡∏∑‡πà‡∏≠, ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢, ‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà, tags)
   - ‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà: ‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à / ‡∏™‡∏≤‡∏ò‡∏≤‡∏£‡∏ì‡∏™‡∏∏‡∏Ç / ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ / ‡∏™‡∏¥‡πà‡∏á‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏° / ‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢ / ‡∏≠‡∏∑‡πà‡∏ô‡πÜ
   - ‡πÅ‡∏ï‡πà‡∏•‡∏∞ project ‡∏°‡∏µ datasets ‡∏´‡∏•‡∏≤‡∏¢‡∏ä‡∏∏‡∏î‡πÑ‡∏î‡πâ
   - ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞: draft / collecting / complete
   - search + filter ‡∏ï‡∏≤‡∏°‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà / tags

2. ‡∏´‡∏ô‡πâ‡∏≤ Data Sources:
   - ‡πÄ‡∏û‡∏¥‡πà‡∏° data source (URL, ‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô, ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó: API/Scrape/Download)
   - ‡∏£‡∏∞‡∏ö‡∏∏ schedule (‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô/‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå/‡πÄ‡∏î‡∏∑‡∏≠‡∏ô/‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß)
   - ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ last fetched, record count
   - ‡∏õ‡∏∏‡πà‡∏° "Fetch Now" ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

3. ‡∏´‡∏ô‡πâ‡∏≤ Dataset Viewer:
   - ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á
   - filter / sort / search ‡πÉ‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á
   - ‡∏õ‡∏∏‡πà‡∏° export ‡πÄ‡∏õ‡πá‡∏ô JSON / CSV
   - ‡πÅ‡∏™‡∏î‡∏á metadata: source, fetched_at, record_count

4. ‡∏´‡∏ô‡πâ‡∏≤ AI Assistant:
   - ‡∏ä‡πà‡∏≠‡∏á chat ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Claude API
   - ‡∏ß‡∏≤‡∏á URL ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ß‡πá‡∏ö‡∏†‡∏≤‡∏Ñ‡∏£‡∏±‡∏ê‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏´‡πâ AI ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á
   - ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ß‡∏¥‡∏ò‡∏µ scrape / API ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
   - ‡∏™‡∏£‡πâ‡∏≤‡∏á scraping code ‡πÉ‡∏´‡πâ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥

‡πÉ‡∏ä‡πâ persistent storage ‡πÄ‡∏Å‡πá‡∏ö projects ‡πÅ‡∏•‡∏∞ datasets ‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏ã‡∏™‡∏ä‡∏±‡∏ô
‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö dark theme ‡πÅ‡∏ö‡∏ö data-engineering / terminal aesthetic
‡πÉ‡∏ä‡πâ monospace font ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö data view
‡∏™‡∏µ accent ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß (#00ff88) ‡∏ö‡∏ô dark background
```

---

## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: Web Scraper Agent (Artifact + Claude API)

### Prompt 2.1 ‚Äî AI-Powered URL Analyzer

```
‡∏™‡∏£‡πâ‡∏≤‡∏á React artifact ‡∏ä‡∏∑‡πà‡∏≠ "Gov Site Analyzer" ‡∏ó‡∏µ‡πà:

1. ‡∏°‡∏µ‡∏ä‡πà‡∏≠‡∏á‡∏Å‡∏£‡∏≠‡∏Å URL ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏£‡∏±‡∏ê
2. ‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏° "‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå" ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Claude API ‡∏û‡∏£‡πâ‡∏≠‡∏° web_search tool
   ‡πÉ‡∏´‡πâ Claude:
   - ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏î‡∏π‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå‡∏ô‡∏±‡πâ‡∏ô
   - ‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤‡∏°‡∏µ API endpoint ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
   - ‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤‡∏°‡∏µ open data / CSV / Excel ‡πÉ‡∏´‡πâ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
   - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô HTML element ‡πÑ‡∏´‡∏ô
   - ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ß‡∏¥‡∏ò‡∏µ scrape ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°

3. ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡πá‡∏ô:
   - Site Overview: ‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô, ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
   - Data Access Method: API / Scrape / Download / CKAN
   - Recommended Approach: ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•
   - Generated Code: Python code ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á
     requests+BeautifulSoup / Selenium / CKAN API)
   - Schema Preview: ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ

4. ‡∏õ‡∏∏‡πà‡∏° "Copy Code" ‡πÅ‡∏•‡∏∞ "Save to Project"

System prompt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Claude API:
---
‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô Government Data Engineering Expert
‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏£‡∏±‡∏ê‡πÑ‡∏ó‡∏¢

‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö URL ‡πÉ‡∏´‡πâ:
1. ‡πÉ‡∏ä‡πâ web_search ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö API/Open Data ‡∏Ç‡∏≠‡∏á‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô
2. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
3. ‡∏™‡∏£‡πâ‡∏≤‡∏á Python code ‡∏ó‡∏µ‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
4. ‡∏£‡∏∞‡∏ö‡∏∏ data schema ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ

‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON format:
{
  "agency": "‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô",
  "data_types": ["‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ"],
  "access_method": "api|scrape|download|ckan",
  "api_endpoints": ["endpoint ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ"],
  "recommended_approach": "‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥",
  "python_code": "‡πÇ‡∏Ñ‡πâ‡∏î Python",
  "expected_schema": {"fields": [...]},
  "notes": "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á / ‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î"
}
---
```

### Prompt 2.2 ‚Äî CKAN API Explorer (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö data.go.th)

```
‡∏™‡∏£‡πâ‡∏≤‡∏á React artifact ‡∏ä‡∏∑‡πà‡∏≠ "Thailand Open Data Explorer" ‡∏ó‡∏µ‡πà:

1. ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö data.go.th CKAN API:
   - Base URL: https://data.go.th/api/3/action/
   - ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á API key ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö read

2. ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå:
   a) ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:
      - ‡∏ä‡πà‡∏≠‡∏á search keyword
      - filter ‡∏ï‡∏≤‡∏°‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô (organization)
      - filter ‡∏ï‡∏≤‡∏°‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà (group)
      - filter ‡∏ï‡∏≤‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö (CSV, API, XLS, JSON)

   b) ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:
      - ‡∏ä‡∏∑‡πà‡∏≠‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• + ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢
      - ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡πÄ‡∏à‡πâ‡∏≤‡∏Ç‡∏≠‡∏á
      - ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ
      - ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
      - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î

   c) ‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:
      - ‡πÅ‡∏™‡∏î‡∏á resources ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
      - preview ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô CSV/JSON)
      - ‡∏õ‡∏∏‡πà‡∏° "‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå"

   d) ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Claude API ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå:
      - ‡∏™‡πà‡∏á metadata ‡∏Ç‡∏≠‡∏á dataset ‡πÉ‡∏´‡πâ Claude
      - Claude ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£‡πÑ‡∏î‡πâ
      - Claude ‡∏™‡∏£‡πâ‡∏≤‡∏á Python code ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î

3. CKAN API Calls ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ:
   - package_search?q={keyword}&rows=20
   - package_show?id={dataset_id}
   - organization_list
   - group_list

‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö UI ‡∏™‡∏∞‡∏≠‡∏≤‡∏î ‡πÉ‡∏ä‡πâ card layout
‡∏™‡∏µ theme ‡πÄ‡∏õ‡πá‡∏ô‡∏ô‡πâ‡∏≥‡πÄ‡∏á‡∏¥‡∏ô-‡∏Ç‡∏≤‡∏ß ‡πÉ‡∏´‡πâ‡∏î‡∏π official / gov-like
```

---

## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: Scraping Code Templates

### Prompt 3.1 ‚Äî ‡∏™‡∏£‡πâ‡∏≤‡∏á Python Scraping Toolkit

```
‡∏™‡∏£‡πâ‡∏≤‡∏á Python project structure ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Government Data Scraper:

project structure:
gov_scraper/
‚îú‚îÄ‚îÄ scrapers/
‚îÇ   ‚îú‚îÄ‚îÄ base.py          # BaseScraper class
‚îÇ   ‚îú‚îÄ‚îÄ ckan_scraper.py  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö data.go.th
‚îÇ   ‚îú‚îÄ‚îÄ bot_scraper.py   # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ‡∏ò‡∏õ‡∏ó.
‚îÇ   ‚îú‚îÄ‚îÄ dbd_scraper.py   # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ‡∏Å‡∏£‡∏°‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à
‚îÇ   ‚îú‚îÄ‚îÄ html_scraper.py  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö scrape HTML ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
‚îÇ   ‚îî‚îÄ‚îÄ pdf_scraper.py   # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å PDF
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ project.py       # Project model
‚îÇ   ‚îú‚îÄ‚îÄ dataset.py       # Dataset model
‚îÇ   ‚îî‚îÄ‚îÄ source.py        # DataSource model
‚îú‚îÄ‚îÄ storage/
‚îÇ   ‚îú‚îÄ‚îÄ local.py         # ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON/CSV ‡πÉ‡∏ô local
‚îÇ   ‚îú‚îÄ‚îÄ sqlite.py        # ‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô SQLite
‚îÇ   ‚îî‚îÄ‚îÄ cloud.py         # ‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô S3/GCS
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ cleaner.py       # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
‚îÇ   ‚îú‚îÄ‚îÄ scheduler.py     # ‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
‚îÇ   ‚îî‚îÄ‚îÄ notifier.py      # ‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà
‚îú‚îÄ‚îÄ config.yaml
‚îú‚îÄ‚îÄ main.py
‚îî‚îÄ‚îÄ requirements.txt

‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î:
1. BaseScraper ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ:
   - rate limiting (‡πÄ‡∏Ñ‡∏≤‡∏£‡∏û robots.txt)
   - retry logic (exponential backoff)
   - logging ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
   - user-agent ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
   - cache layer (‡πÑ‡∏°‡πà‡∏î‡∏∂‡∏á‡∏ã‡πâ‡∏≥‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô)
   - error handling ‡∏ó‡∏µ‡πà‡∏î‡∏µ

2. ‡πÅ‡∏ï‡πà‡∏•‡∏∞ scraper ‡∏ï‡πâ‡∏≠‡∏á implement:
   - fetch(): ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö
   - parse(): ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô structured data
   - validate(): ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
   - save(): ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á storage

3. ‡πÉ‡∏´‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ô‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á ‡∏û‡∏£‡πâ‡∏≠‡∏° docstring ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
```

### Prompt 3.2 ‚Äî CKAN Scraper ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö data.go.th

```
‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô Python class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å data.go.th CKAN API:

class CKANScraper:
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏Å‡∏•‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡∏¥‡∏î‡∏†‡∏≤‡∏Ñ‡∏£‡∏±‡∏ê"""

    base_url = "https://data.go.th/api/3/action/"

    methods ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ:
    1. search_datasets(keyword, org, group, format, limit)
       - ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å package_search
       - return list of dataset metadata

    2. get_dataset(dataset_id)
       - ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å package_show
       - return full dataset with resources

    3. download_resource(resource_id, save_path)
       - ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå (CSV, XLS, JSON)
       - auto-detect encoding (Thai encoding: TIS-620, UTF-8)
       - return DataFrame

    4. list_organizations()
       - ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å organization_list?all_fields=true
       - return list of orgs with metadata

    5. list_groups()
       - ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å group_list?all_fields=true
       - return list of categories

    6. bulk_download(keyword, save_dir)
       - ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ + ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏∏‡∏Å dataset ‡∏ó‡∏µ‡πà match
       - rate limiting 1 req/sec
       - progress bar ‡∏î‡πâ‡∏ß‡∏¢ tqdm

‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á ‡πÄ‡∏ä‡πà‡∏ô:
- ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ô‡∏¥‡∏ï‡∏¥‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•‡∏à‡∏≤‡∏Å DBD
- ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡πÅ‡∏•‡∏Å‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å BOT
- ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏Å‡∏£‡∏à‡∏≤‡∏Å NSO
```

### Prompt 3.3 ‚Äî HTML Scraper ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå‡∏†‡∏≤‡∏Ñ‡∏£‡∏±‡∏ê‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ

```
‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô Python class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö scrape ‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå‡∏†‡∏≤‡∏Ñ‡∏£‡∏±‡∏ê‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ API:

class GovHTMLScraper(BaseScraper):
    """Scrape ‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå‡∏†‡∏≤‡∏Ñ‡∏£‡∏±‡∏ê‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ API"""

    ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡πâ‡∏≤‡∏ó‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏°‡∏∑‡∏≠:
    1. Thai encoding (TIS-620 / Windows-874 / UTF-8)
    2. ‡πÄ‡∏ß‡πá‡∏ö‡πÄ‡∏Å‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ table layout
    3. ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô iframe
    4. JavaScript rendering (‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ Selenium)
    5. PDF ‡∏ó‡∏µ‡πà embed ‡πÉ‡∏ô‡πÄ‡∏ß‡πá‡∏ö
    6. CAPTCHA (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ß‡∏¥‡∏ò‡∏µ‡∏£‡∏±‡∏ö‡∏°‡∏∑‡∏≠)

    methods:
    1. fetch_page(url, encoding='auto')
       - auto-detect encoding
       - handle redirects
       - return BeautifulSoup object

    2. extract_tables(soup)
       - ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å HTML tables
       - handle merged cells (colspan/rowspan)
       - return list of DataFrames

    3. extract_links(soup, pattern)
       - ‡∏î‡∏∂‡∏á links ‡∏ó‡∏µ‡πà match regex pattern
       - ‡πÄ‡∏ä‡πà‡∏ô links ‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡πÑ‡∏ü‡∏•‡πå PDF, CSV, XLS

    4. extract_text_content(soup, selectors)
       - ‡∏î‡∏∂‡∏á text ‡∏à‡∏≤‡∏Å CSS selectors ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
       - clean whitespace + Thai text normalization

    5. crawl_paginated(base_url, page_param, max_pages)
       - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ scrape ‡∏´‡∏•‡∏≤‡∏¢‡∏´‡∏ô‡πâ‡∏≤
       - auto-detect pagination pattern

    6. download_files(urls, save_dir, file_type)
       - ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡∏•‡∏∞‡∏ï‡∏±‡∏ß
       - rename ‡∏î‡πâ‡∏ß‡∏¢ convention: {agency}_{date}_{type}.{ext}

‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏à‡∏£‡∏¥‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:
- scrape ‡∏Ç‡πà‡∏≤‡∏ß‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô
- ‡∏î‡∏∂‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏à‡∏≤‡∏Å HTML
- ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î PDF ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏õ‡∏µ
```

---

## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 4: Data Schema & Storage

### Prompt 4.1 ‚Äî ‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö Database Schema

```
‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö database schema (SQLite/PostgreSQL) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏≤‡∏Ñ‡∏£‡∏±‡∏ê:

‡∏ï‡∏≤‡∏£‡∏≤‡∏á:
1. projects
   - id, name, description, category, tags[], status,
     created_at, updated_at

2. data_sources
   - id, project_id (FK), name, agency, url,
     access_type (api/scrape/download/ckan),
     schedule (daily/weekly/monthly/once),
     config_json, last_fetched_at, status

3. datasets
   - id, source_id (FK), project_id (FK),
     name, description, version, record_count,
     schema_json, fetched_at, file_path, file_format

4. records (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö structured data)
   - id, dataset_id (FK), data_json,
     created_at, source_url

5. fetch_logs
   - id, source_id (FK), started_at, completed_at,
     status (success/error/partial),
     records_fetched, error_message

6. tags
   - id, name, category

7. project_tags (many-to-many)
   - project_id, tag_id

‡πÉ‡∏´‡πâ:
- SQL CREATE statements
- Python SQLAlchemy models
- Index ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö query ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ö‡πà‡∏≠‡∏¢
- Migration script
- ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡∏•‡∏≠‡∏á
```

### Prompt 4.2 ‚Äî Data Cleaning Pipeline

```
‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô Python module ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏≤‡∏Ñ‡∏£‡∏±‡∏ê‡πÑ‡∏ó‡∏¢:

class ThaiGovDataCleaner:
    """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏†‡∏≤‡∏Ñ‡∏£‡∏±‡∏ê‡πÑ‡∏ó‡∏¢"""

    ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢:
    1. Encoding: TIS-620 ‚Üî UTF-8 conversion
    2. ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà: ‡∏û.‡∏®. ‚Üî ‡∏Ñ.‡∏®. conversion
       - "25 ‡∏°.‡∏Ñ. 2567" ‚Üí "2024-01-25"
       - "25/01/67" ‚Üí "2024-01-25"
       - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡πÄ‡∏•‡∏Ç‡πÑ‡∏ó‡∏¢ ‡πí‡πï‡πñ‡πó ‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏Ç‡∏≠‡∏≤‡∏£‡∏ö‡∏¥‡∏Å
    3. ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç: ‡πÄ‡∏•‡∏Ç‡πÑ‡∏ó‡∏¢ ‚Üî ‡∏≠‡∏≤‡∏£‡∏ö‡∏¥‡∏Å
       - "‡πë,‡πí‡πì‡πî.‡πï‡πñ" ‚Üí 1234.56
    4. ‡∏ä‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î: normalize (‡∏Å‡∏ó‡∏°./‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏Ø/‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£)
    5. ‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô: normalize ‡∏ä‡∏∑‡πà‡∏≠‡∏¢‡πà‡∏≠ ‚Üî ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡πá‡∏°
    6. Missing values: "-", "N/A", "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", ""
    7. HTML entities: &amp; ‚Üí &, &nbsp; ‚Üí " "
    8. White space: \xa0, zero-width space
    9. ‡πÄ‡∏•‡∏Ç‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏ï‡∏±‡∏ß: format validation (‡πÄ‡∏•‡∏Ç 13 ‡∏´‡∏•‡∏±‡∏Å etc.)

    methods:
    - clean_encoding(text)
    - convert_thai_date(date_str, output_format)
    - convert_thai_numbers(text)
    - normalize_province(name)
    - normalize_agency(name)
    - clean_currency(text) ‚Üí float
    - clean_dataframe(df, column_config)
    - validate_id_number(id_str, id_type)

‡πÉ‡∏´‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ó‡∏∏‡∏Å‡∏Å‡∏£‡∏ì‡∏µ‡∏Ç‡πâ‡∏≤‡∏á‡∏ï‡πâ‡∏ô
‡∏û‡∏£‡πâ‡∏≠‡∏° unit tests
```

---

## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 5: All-in-One Artifact

### Prompt 5.1 ‚Äî Government Data Hub (‡∏£‡∏ß‡∏°‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á)

```
‡∏™‡∏£‡πâ‡∏≤‡∏á React artifact ‡∏ä‡∏∑‡πà‡∏≠ "Gov Data Hub üèõÔ∏è" ‡∏£‡∏ß‡∏°‡∏ó‡∏∏‡∏Å‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÉ‡∏ô‡πÅ‡∏≠‡∏õ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß:

Tab 1 - "‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå" (Project Manager):
  - ‡∏™‡∏£‡πâ‡∏≤‡∏á / ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç / ‡∏•‡∏ö projects
  - ‡πÅ‡∏ï‡πà‡∏•‡∏∞ project ‡∏°‡∏µ datasets ‡∏´‡∏•‡∏≤‡∏¢‡∏ä‡∏∏‡∏î
  - tag system ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏î‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà
  - search / filter / sort

Tab 2 - "‡∏™‡∏≥‡∏£‡∏ß‡∏à" (Data Explorer):
  - ‡∏Å‡∏£‡∏≠‡∏Å URL ‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏£‡∏±‡∏ê
  - ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Claude API + web_search ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ß‡πá‡∏ö
  - ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå: ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•, ‡∏ß‡∏¥‡∏ò‡∏µ‡∏î‡∏∂‡∏á, API endpoints
  - ‡∏™‡∏£‡πâ‡∏≤‡∏á Python scraping code ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
  - ‡∏õ‡∏∏‡πà‡∏° "‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå"

Tab 3 - "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•" (Data Viewer):
  - ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏î‡∏π datasets ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ project
  - ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏µ‡πà sort / filter / search ‡πÑ‡∏î‡πâ
  - ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô (count, unique, min, max)
  - export ‡πÄ‡∏õ‡πá‡∏ô JSON / CSV
  - ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á

Tab 4 - "AI ‡∏ä‡πà‡∏ß‡∏¢‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå" (AI Assistant):
  - chat interface ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Claude API
  - ‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå
  - ‡πÉ‡∏´‡πâ Claude ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå / ‡∏´‡∏≤ pattern / ‡∏™‡∏£‡∏∏‡∏õ insight
  - ‡∏™‡∏£‡πâ‡∏≤‡∏á visualization code

Tab 5 - "‡∏Ñ‡∏•‡∏±‡∏á‡πÇ‡∏Ñ‡πâ‡∏î" (Code Snippets):
  - templates ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏£‡∏π‡∏õ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô:
    ‚Ä¢ data.go.th (CKAN API)
    ‚Ä¢ BOT API (‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡πÅ‡∏•‡∏Å‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô, ‡∏î‡∏≠‡∏Å‡πÄ‡∏ö‡∏µ‡πâ‡∏¢)
    ‚Ä¢ DBD Open Data (‡∏ô‡∏¥‡∏ï‡∏¥‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•)
    ‚Ä¢ HTML scraper template
    ‚Ä¢ PDF extractor template
  - ‡∏Å‡∏î copy ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢
  - Claude AI ‡∏ä‡πà‡∏ß‡∏¢ customize ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏≤‡∏° use case

‡πÉ‡∏ä‡πâ persistent storage ‡πÄ‡∏Å‡πá‡∏ö‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á
Demo data: ‡∏à‡∏≥‡∏•‡∏≠‡∏á 3 projects ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:
  1. "‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡πÑ‡∏ó‡∏¢" - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ô‡∏¥‡∏ï‡∏¥‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•‡∏à‡∏≤‡∏Å DBD
  2. "‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à" - GDP, CPI, ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡πÅ‡∏•‡∏Å‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô ‡∏à‡∏≤‡∏Å BOT
  3. "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏¥‡πà‡∏á‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°" - ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏≠‡∏≤‡∏Å‡∏≤‡∏®, PM2.5 ‡∏à‡∏≤‡∏Å PCD

Design: dark theme, monospace ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö code/data
‡∏™‡∏µ accent: ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß‡∏ô‡∏µ‡∏≠‡∏≠‡∏ô (#00ff88) + cyan (#00d4ff)
font: IBM Plex Mono ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö code, IBM Plex Sans Thai ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö UI
```

---

## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 6: Code Snippets ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏£‡∏π‡∏õ

### 6.1 ‚Äî data.go.th CKAN API

```python
"""
Prompt: ‡∏™‡∏£‡πâ‡∏≤‡∏á Python snippet ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å data.go.th
‡∏î‡πâ‡∏ß‡∏¢ CKAN API ‡∏û‡∏£‡πâ‡∏≠‡∏° error handling ‡πÅ‡∏•‡∏∞ Thai encoding support
"""

import requests
import pandas as pd
from io import StringIO

class DataGoTH:
    BASE = "https://data.go.th/api/3/action/"

    def search(self, keyword, rows=10):
        """‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""
        r = requests.get(f"{self.BASE}package_search",
                         params={"q": keyword, "rows": rows})
        return r.json()["result"]["results"]

    def get_dataset(self, dataset_id):
        """‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""
        r = requests.get(f"{self.BASE}package_show",
                         params={"id": dataset_id})
        return r.json()["result"]

    def download_csv(self, resource_url):
        """‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î CSV ‡∏û‡∏£‡πâ‡∏≠‡∏° handle Thai encoding"""
        r = requests.get(resource_url)
        # Auto-detect encoding
        for enc in ['utf-8', 'tis-620', 'cp874']:
            try:
                text = r.content.decode(enc)
                return pd.read_csv(StringIO(text))
            except (UnicodeDecodeError, pd.errors.ParserError):
                continue
        raise ValueError("Cannot decode file")

    def list_by_org(self, org_name, limit=50):
        """‡∏î‡∏∂‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô"""
        r = requests.get(f"{self.BASE}package_search",
                         params={"fq": f"organization:{org_name}",
                                 "rows": limit})
        return r.json()["result"]["results"]

# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
api = DataGoTH()
datasets = api.search("‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ô‡∏¥‡∏ï‡∏¥‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•")
for ds in datasets:
    print(f"üì¶ {ds['title']}")
    print(f"   ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô: {ds['organization']['title']}")
    print(f"   ‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£: {len(ds['resources'])} ‡πÑ‡∏ü‡∏•‡πå")
```

### 6.2 ‚Äî BOT API (‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡πÅ‡∏•‡∏Å‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô)

```python
"""
Prompt: ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô Python snippet ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏∂‡∏á‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡πÅ‡∏•‡∏Å‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô
‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡∏à‡∏≤‡∏Å ‡∏ò‡∏õ‡∏ó. API
"""

import requests
import pandas as pd

class BOTAPI:
    BASE = "https://apigw1.bot.or.th/bot/public/v2"

    def __init__(self, api_key):
        self.headers = {
            "X-IBM-Client-Id": api_key,
            "accept": "application/json"
        }

    def get_exchange_rate(self, start_date, end_date, currency="USD"):
        """‡∏î‡∏∂‡∏á‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡πÅ‡∏•‡∏Å‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô"""
        url = f"{self.BASE}/Fin_ExchangeRateAll/"
        params = {
            "start_period": start_date,  # YYYY-MM-DD
            "end_period": end_date,
            "currency": currency
        }
        r = requests.get(url, headers=self.headers, params=params)
        return r.json()

    def get_interest_rate(self, start_date, end_date):
        """‡∏î‡∏∂‡∏á‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏î‡∏≠‡∏Å‡πÄ‡∏ö‡∏µ‡πâ‡∏¢‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢"""
        url = f"{self.BASE}/Fin_PolicyRate/"
        params = {
            "start_period": start_date,
            "end_period": end_date
        }
        r = requests.get(url, headers=self.headers, params=params)
        return r.json()

# ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏°‡∏±‡∏Ñ‡∏£ API key ‡∏ó‡∏µ‡πà https://apiportal.bot.or.th/
```

### 6.3 ‚Äî General HTML Scraper

```python
"""
Prompt: ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô Python snippet ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö scrape ‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå‡∏†‡∏≤‡∏Ñ‡∏£‡∏±‡∏ê
‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ API ‡πÇ‡∏î‡∏¢‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Thai encoding
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import re

class GovSiteScraper:
    def __init__(self, delay=1.0):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'GovDataBot/1.0 (research purposes)'
        })
        self.delay = delay

    def fetch(self, url):
        """‡∏î‡∏∂‡∏á HTML ‡∏û‡∏£‡πâ‡∏≠‡∏° auto-detect encoding"""
        time.sleep(self.delay)  # Rate limiting
        r = self.session.get(url, timeout=30)
        # Auto-detect Thai encoding
        if r.encoding and 'iso-8859' in r.encoding.lower():
            r.encoding = 'tis-620'
        return BeautifulSoup(r.text, 'html.parser')

    def extract_tables(self, soup):
        """‡∏î‡∏∂‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏õ‡πá‡∏ô DataFrames"""
        tables = []
        for table in soup.find_all('table'):
            rows = []
            for tr in table.find_all('tr'):
                cells = [td.get_text(strip=True)
                         for td in tr.find_all(['td', 'th'])]
                if cells:
                    rows.append(cells)
            if rows:
                df = pd.DataFrame(rows[1:], columns=rows[0])
                tables.append(df)
        return tables

    def extract_pdf_links(self, soup, base_url):
        """‡∏î‡∏∂‡∏á links ‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡πÑ‡∏ü‡∏•‡πå PDF"""
        links = []
        for a in soup.find_all('a', href=True):
            href = a['href']
            if href.lower().endswith('.pdf'):
                if not href.startswith('http'):
                    href = base_url.rstrip('/') + '/' + href.lstrip('/')
                links.append({
                    'url': href,
                    'text': a.get_text(strip=True)
                })
        return links

    def crawl_pages(self, base_url, page_param='page',
                    start=1, max_pages=10):
        """Scrape ‡∏´‡∏•‡∏≤‡∏¢‡∏´‡∏ô‡πâ‡∏≤"""
        all_data = []
        for page in range(start, start + max_pages):
            url = f"{base_url}?{page_param}={page}"
            soup = self.fetch(url)
            tables = self.extract_tables(soup)
            if not tables:
                break
            all_data.extend(tables)
            print(f"  ‚úÖ Page {page}: {len(tables)} tables found")
        return all_data

# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
scraper = GovSiteScraper(delay=2.0)
soup = scraper.fetch("https://www.example.go.th/data")
tables = scraper.extract_tables(soup)
pdfs = scraper.extract_pdf_links(soup, "https://www.example.go.th")
```

---

## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 7: Best Practices & ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á

### ‡∏Ç‡πâ‡∏≠‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏à‡∏£‡∏¥‡∏¢‡∏ò‡∏£‡∏£‡∏°

| ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ | ‡πÅ‡∏ô‡∏ß‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥ |
|---|---|
| **robots.txt** | ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÄ‡∏Ñ‡∏≤‡∏£‡∏û‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á |
| **Rate limiting** | ‚â§ 1 request/second ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö scraping |
| **‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏ß‡∏ô‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•** | ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏ß‡∏ô‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï (PDPA) |
| **Open Data License** | ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö license ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• |
| **Terms of Service** | ‡∏≠‡πà‡∏≤‡∏ô ToS ‡∏Å‡πà‡∏≠‡∏ô scrape ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á |
| **‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå** | ‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡∏à‡∏±‡∏¢/‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå/‡∏™‡∏≤‡∏ò‡∏≤‡∏£‡∏ì‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå |
| **Attribution** | ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤‡πÄ‡∏™‡∏°‡∏≠ |

### ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡∏¥‡∏ò‡∏µ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏à‡∏≤‡∏Å‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏°‡∏≤‡∏Å‡∏™‡∏∏‡∏î)

```
1. Official API (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)           ‚Üí ‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£, ‡∏°‡∏µ schema ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
2. CKAN API (data.go.th)         ‚Üí ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô, ‡∏°‡∏µ metadata
3. Direct download (CSV/Excel)    ‚Üí ‡∏á‡πà‡∏≤‡∏¢, ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á parse HTML
4. HTML scraping (BeautifulSoup)  ‚Üí ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ó‡∏≤‡∏á‡∏≠‡∏∑‡πà‡∏ô
5. JavaScript rendering (Selenium)‚Üí ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏ß‡πá‡∏ö render ‡∏î‡πâ‡∏ß‡∏¢ JS
6. PDF extraction (tabula-py)     ‚Üí ‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
```

### Monitoring & Scheduling

```
Prompt: ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô Python script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö schedule
‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ ‡∏î‡πâ‡∏ß‡∏¢ APScheduler:

- ‡∏î‡∏∂‡∏á‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡πÅ‡∏•‡∏Å‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô BOT ‡∏ó‡∏∏‡∏Å‡∏ß‡∏±‡∏ô 09:00
- ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ô‡∏¥‡∏ï‡∏¥‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏• DBD ‡∏ó‡∏∏‡∏Å‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 1
- ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏à‡∏≤‡∏Å‡∏£‡∏≤‡∏ä‡∏Å‡∏¥‡∏à‡∏à‡∏≤‡∏ô‡∏∏‡πÄ‡∏ö‡∏Å‡∏©‡∏≤‡∏ó‡∏∏‡∏Å‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå
- ‡∏™‡πà‡∏á notification ‡πÄ‡∏Ç‡πâ‡∏≤ LINE Notify ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà
- ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å log ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
- retry 3 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ñ‡πâ‡∏≤ error
```

---

## ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥

```
Step 1: ‡πÉ‡∏ä‡πâ Prompt 5.1 ‡∏™‡∏£‡πâ‡∏≤‡∏á All-in-One Artifact
        ‚Üí ‡πÑ‡∏î‡πâ dashboard + demo data ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÉ‡∏ô Claude

Step 2: ‡πÉ‡∏ä‡πâ Prompt 2.2 ‡∏™‡∏£‡πâ‡∏≤‡∏á CKAN Explorer
        ‚Üí ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏≤‡∏Å data.go.th

Step 3: ‡πÉ‡∏ä‡πâ Prompt 2.1 ‡πÉ‡∏´‡πâ AI ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ß‡πá‡∏ö‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        ‚Üí ‡πÑ‡∏î‡πâ code ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏£‡∏π‡∏õ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏ß‡πá‡∏ö

Step 4: ‡πÉ‡∏ä‡πâ Prompt 3.1-3.3 ‡∏™‡∏£‡πâ‡∏≤‡∏á Python project
        ‚Üí ‡∏ô‡∏≥ code ‡πÑ‡∏õ‡∏£‡∏±‡∏ô‡∏à‡∏£‡∏¥‡∏á‡∏ö‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á

Step 5: ‡πÉ‡∏ä‡πâ Prompt 4.1-4.2 ‡∏™‡∏£‡πâ‡∏≤‡∏á database + cleaner
        ‚Üí ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö

Step 6: ‡∏ï‡∏±‡πâ‡∏á scheduler ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
        ‚Üí ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ï‡∏•‡∏≠‡∏î‡πÄ‡∏ß‡∏•‡∏≤
```

> üí° **‡πÄ‡∏Ñ‡∏•‡πá‡∏î‡∏•‡∏±‡∏ö**: ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å data.go.th ‡πÄ‡∏™‡∏°‡∏≠ ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏°‡∏µ API ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô CKAN
> ‡∏î‡∏∂‡∏á‡∏á‡πà‡∏≤‡∏¢ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á scrape HTML ‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏µ metadata ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
> ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢‡∏Ç‡∏¢‡∏≤‡∏¢‡πÑ‡∏õ‡πÄ‡∏ß‡πá‡∏ö‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á scrape ‡πÄ‡∏≠‡∏á
